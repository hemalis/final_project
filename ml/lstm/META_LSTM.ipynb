{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance\n",
        "!pip install yahoofinancials"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMM5oRDNAIy-",
        "outputId": "81502029-2445-4c1f-f99b-83c6928e3fbf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.7/dist-packages (0.1.85)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Requirement already satisfied: lxml>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from yfinance) (4.9.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.6)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.28.1)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: yahoofinancials in /usr/local/lib/python3.7/dist-packages (1.6)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from yahoofinancials) (4.6.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from yahoofinancials) (2022.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aEHgGMtF8NLd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time as tm\n",
        "import datetime \n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "# Data preparation\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from collections import deque\n",
        "\n",
        "# AI\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout\n",
        "\n",
        "# Graphics library\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import yfinance as yf\n",
        "from yahoofinancials import YahooFinancials\n",
        "import time\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#time to pull the data\n",
        "start_date =datetime.datetime.today().strftime('%Y-%m-%d')\n",
        "end_date = (datetime.datetime.today() - datetime.timedelta(days=1100)).strftime('%Y-%m-%d')"
      ],
      "metadata": {
        "id": "wt9C0HKdAyrU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9VtXlmLY-7h"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calling API \n",
        "data=YahooFinancials('meta').get_historical_price_data(end_date,start_date,\"daily\")\n",
        "#into the dataframe\n",
        "meta_df = pd.DataFrame(data['META']['prices'])\n",
        "meta_df =meta_df.drop(['high','low','open','volume','date','adjclose'], axis=1)\n",
        "meta_df.rename(columns={'formatted_date':'date'},inplace=True)\n",
        "meta_df['date']=pd.to_datetime(meta_df['date'])"
      ],
      "metadata": {
        "id": "J6IhlfSZAP90"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "r-Mb0wsxXipU"
      },
      "outputs": [],
      "source": [
        "# SETTINGS\n",
        "\n",
        "# Window size or the sequence length, 7 (1 week)\n",
        "N_STEPS = 7\n",
        "\n",
        "# Lookup steps, 1 is the next day, 3 = after tomorrow\n",
        "LOOKUP_STEPS = [1, 2, 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FRC5O4aUckB3"
      },
      "outputs": [],
      "source": [
        "# Scale data for ML engine\n",
        "scaler = MinMaxScaler()\n",
        "meta_df['scaled_close'] = scaler.fit_transform(np.expand_dims(meta_df['close'].values, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def PrepareData(days):\n",
        "    df = meta_df.copy()\n",
        "    df['future'] = df['scaled_close'].shift(-days)\n",
        "    last_sequence = np.array(df[['scaled_close']].tail(days))\n",
        "    df.dropna(inplace=True)\n",
        "    sequence_data = []\n",
        "    sequences = deque(maxlen=N_STEPS)\n",
        "    \n",
        "    for entry, target in zip(df[['scaled_close'] + ['date']].values, df['future'].values):\n",
        "        sequences.append(entry)\n",
        "        if len(sequences) == N_STEPS:\n",
        "            sequence_data.append([np.array(sequences), target])\n",
        "            \n",
        "    last_sequence = list([s[:len(['scaled_close'])] for s in sequences]) + list(last_sequence)\n",
        "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
        "\n",
        "    # construct the X's and Y's\n",
        "    X, Y = [], []\n",
        "    for seq, target in sequence_data:\n",
        "        X.append(seq)\n",
        "        Y.append(target)\n",
        "\n",
        "   # convert to numpy arrays\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    return df, last_sequence, X, Y"
      ],
      "metadata": {
        "id": "3ww7vT6zGX4k"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PrepareData(3) # 3 days\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBnVZJ47W32T",
        "outputId": "2507a673-c325-4e0d-a9d0-9073a9d3e000"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(          close       date  scaled_close    future\n",
              " 0    190.419998 2019-11-07      0.346132  0.359941\n",
              " 1    190.839996 2019-11-08      0.347564  0.355577\n",
              " 2    189.610001 2019-11-11      0.343370  0.355440\n",
              " 3    194.470001 2019-11-12      0.359941  0.362090\n",
              " 4    193.190002 2019-11-13      0.355577  0.369932\n",
              " ..          ...        ...           ...       ...\n",
              " 751   95.199997 2022-11-01      0.021448  0.006410\n",
              " 752   90.540001 2022-11-02      0.005558  0.026631\n",
              " 753   88.910004 2022-11-03      0.000000  0.025778\n",
              " 754   90.790001 2022-11-04      0.006410  0.042827\n",
              " 755   96.720001 2022-11-07      0.026631  0.078290\n",
              " \n",
              " [756 rows x 4 columns], array([[0.0350871 ],\n",
              "        [0.01449177],\n",
              "        [0.02144779],\n",
              "        [0.00555801],\n",
              "        [0.        ],\n",
              "        [0.00641047],\n",
              "        [0.02663074],\n",
              "        [0.02577829],\n",
              "        [0.04282742],\n",
              "        [0.07828963]], dtype=float32), array([[[0.3461315453662177, Timestamp('2019-11-07 00:00:00')],\n",
              "         [0.34756366656742843, Timestamp('2019-11-08 00:00:00')],\n",
              "         [0.3433695936188945, Timestamp('2019-11-11 00:00:00')],\n",
              "         ...,\n",
              "         [0.35557678141569227, Timestamp('2019-11-13 00:00:00')],\n",
              "         [0.35544035918900474, Timestamp('2019-11-14 00:00:00')],\n",
              "         [0.3620895639494016, Timestamp('2019-11-15 00:00:00')]],\n",
              " \n",
              "        [[0.34756366656742843, Timestamp('2019-11-08 00:00:00')],\n",
              "         [0.3433695936188945, Timestamp('2019-11-11 00:00:00')],\n",
              "         [0.3599413561326682, Timestamp('2019-11-12 00:00:00')],\n",
              "         ...,\n",
              "         [0.35544035918900474, Timestamp('2019-11-14 00:00:00')],\n",
              "         [0.3620895639494016, Timestamp('2019-11-15 00:00:00')],\n",
              "         [0.36993212499938866, Timestamp('2019-11-18 00:00:00')]],\n",
              " \n",
              "        [[0.3433695936188945, Timestamp('2019-11-11 00:00:00')],\n",
              "         [0.3599413561326682, Timestamp('2019-11-12 00:00:00')],\n",
              "         [0.35557678141569227, Timestamp('2019-11-13 00:00:00')],\n",
              "         ...,\n",
              "         [0.3620895639494016, Timestamp('2019-11-15 00:00:00')],\n",
              "         [0.36993212499938866, Timestamp('2019-11-18 00:00:00')],\n",
              "         [0.3764790391046873, Timestamp('2019-11-19 00:00:00')]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0.13949604526429127, Timestamp('2022-10-26 00:00:00')],\n",
              "         [0.03079073590061715, Timestamp('2022-10-27 00:00:00')],\n",
              "         [0.03508709950424932, Timestamp('2022-10-28 00:00:00')],\n",
              "         ...,\n",
              "         [0.021447790506240905, Timestamp('2022-11-01 00:00:00')],\n",
              "         [0.005558009051318147, Timestamp('2022-11-02 00:00:00')],\n",
              "         [0.0, Timestamp('2022-11-03 00:00:00')]],\n",
              " \n",
              "        [[0.03079073590061715, Timestamp('2022-10-27 00:00:00')],\n",
              "         [0.03508709950424932, Timestamp('2022-10-28 00:00:00')],\n",
              "         [0.014491765810383916, Timestamp('2022-10-31 00:00:00')],\n",
              "         ...,\n",
              "         [0.005558009051318147, Timestamp('2022-11-02 00:00:00')],\n",
              "         [0.0, Timestamp('2022-11-03 00:00:00')],\n",
              "         [0.0064104658636936485, Timestamp('2022-11-04 00:00:00')]],\n",
              " \n",
              "        [[0.03508709950424932, Timestamp('2022-10-28 00:00:00')],\n",
              "         [0.014491765810383916, Timestamp('2022-10-31 00:00:00')],\n",
              "         [0.021447790506240905, Timestamp('2022-11-01 00:00:00')],\n",
              "         ...,\n",
              "         [0.0, Timestamp('2022-11-03 00:00:00')],\n",
              "         [0.0064104658636936485, Timestamp('2022-11-04 00:00:00')],\n",
              "         [0.026630742493837845, Timestamp('2022-11-07 00:00:00')]]],\n",
              "       dtype=object), array([0.37030721, 0.37173933, 0.37477413, 0.37808161, 0.37528558,\n",
              "        0.38561735, 0.38438981, 0.37777474, 0.37477413, 0.37439904,\n",
              "        0.37661541, 0.38237803, 0.38336685, 0.38176423, 0.38650389,\n",
              "        0.36771576, 0.35871382, 0.37170525, 0.37330787, 0.38732226,\n",
              "        0.39946124, 0.40027962, 0.3998704 , 0.396256  , 0.40536023,\n",
              "        0.40641732, 0.39383505, 0.39669929, 0.4121458 , 0.40836089,\n",
              "        0.42176154, 0.42333003, 0.43069527, 0.44119755, 0.44037917,\n",
              "        0.45350702, 0.443789  , 0.45091552, 0.45302965, 0.45429127,\n",
              "        0.4519044 , 0.45149524, 0.44617586, 0.43997001, 0.42950181,\n",
              "        0.4394585 , 0.45800797, 0.41129335, 0.38531048, 0.39308488,\n",
              "        0.4123163 , 0.41327105, 0.41579434, 0.42084087, 0.42333003,\n",
              "        0.40331436, 0.41548742, 0.42360282, 0.42714902, 0.43949263,\n",
              "        0.43843559, 0.42851298, 0.41350971, 0.38125278, 0.36778397,\n",
              "        0.36925017, 0.34384697, 0.3531217 , 0.36665872, 0.33068503,\n",
              "        0.3507007 , 0.32822995, 0.31431785, 0.27479797, 0.30442937,\n",
              "        0.27732126, 0.22354827, 0.27745763, 0.1947011 , 0.20632863,\n",
              "        0.19794048, 0.21897911, 0.20738567, 0.20182768, 0.24574622,\n",
              "        0.22948138, 0.25379342, 0.23145904, 0.26269307, 0.26559144,\n",
              "        0.2410407 , 0.23623283, 0.22255939, 0.26132916, 0.27251339,\n",
              "        0.29109694, 0.29419989, 0.29283593, 0.30436116, 0.30026938,\n",
              "        0.2978143 , 0.3080097 , 0.30459987, 0.27923075, 0.31837555,\n",
              "        0.32809358, 0.34493814, 0.33617486, 0.32052376, 0.35898661,\n",
              "        0.39485801, 0.38653802, 0.39673337, 0.4029052 , 0.40767894,\n",
              "        0.41719233, 0.42090908, 0.42373919, 0.41323697, 0.39618784,\n",
              "        0.40201861, 0.41589663, 0.42377333, 0.4363556 , 0.48099022,\n",
              "        0.48583217, 0.49783478, 0.48859412, 0.47816006, 0.46561192,\n",
              "        0.46435025, 0.4876053 , 0.49036725, 0.4816381 , 0.46844203,\n",
              "        0.4837181 , 0.48586625, 0.51065571, 0.50404064, 0.46209975,\n",
              "        0.47625057, 0.48961708, 0.50035802, 0.49994885, 0.5013469 ,\n",
              "        0.51106487, 0.51253113, 0.52282882, 0.49480003, 0.50046031,\n",
              "        0.43362773, 0.44917653, 0.47110175, 0.50683672, 0.49275412,\n",
              "        0.51614553, 0.51812324, 0.52739797, 0.53053501, 0.53247864,\n",
              "        0.51178096, 0.51427012, 0.51614553, 0.5183619 , 0.52211273,\n",
              "        0.53367204, 0.52115798, 0.51474749, 0.48995809, 0.48351351,\n",
              "        0.49302691, 0.48150168, 0.49231082, 0.49643674, 0.56180312,\n",
              "        0.55597234, 0.54870939, 0.54628839, 0.60139122, 0.61216628,\n",
              "        0.59361681, 0.57019132, 0.5830123 , 0.58782007, 0.58761548,\n",
              "        0.58734274, 0.59136631, 0.59221877, 0.61410991, 0.60729025,\n",
              "        0.62222531, 0.65437996, 0.73311286, 0.6966618 , 0.69816213,\n",
              "        0.69659364, 0.70423162, 0.72830499, 0.68950114, 0.66089274,\n",
              "        0.62144102, 0.63017017, 0.61097282, 0.60592624, 0.60435775,\n",
              "        0.62573743, 0.59538989, 0.56572445, 0.55791592, 0.54298086,\n",
              "        0.56548574, 0.54594744, 0.54768644, 0.56572445, 0.57254411,\n",
              "        0.58949095, 0.58986598, 0.6059945 , 0.58318275, 0.59924301,\n",
              "        0.57881818, 0.57697684, 0.59620832, 0.5985611 , 0.63709211,\n",
              "        0.63842199, 0.62369151, 0.60630137, 0.60360758, 0.58816107,\n",
              "        0.60916562, 0.64725343, 0.64517338, 0.66791698, 0.64172943,\n",
              "        0.66280224, 0.60954075, 0.65441399, 0.59399184, 0.58802465,\n",
              "        0.60145937, 0.67674842, 0.70164012, 0.69730967, 0.64738975,\n",
              "        0.62577146, 0.63958132, 0.63480748, 0.64118395, 0.64803763,\n",
              "        0.63453474, 0.62420297, 0.6275105 , 0.6164627 , 0.61213215,\n",
              "        0.64108165, 0.63654653, 0.64411635, 0.6412521 , 0.6739182 ,\n",
              "        0.67722574, 0.65789208, 0.65056097, 0.67061067, 0.66317727,\n",
              "        0.64449148, 0.64176356, 0.62959045, 0.63177279, 0.6364101 ,\n",
              "        0.63681937, 0.63276167, 0.63930848, 0.62699905, 0.60756299,\n",
              "        0.61104098, 0.60862003, 0.64135439, 0.64060423, 0.62386196,\n",
              "        0.62826067, 0.6138712 , 0.62079314, 0.59467385, 0.61318919,\n",
              "        0.60919975, 0.57261227, 0.55300576, 0.55488117, 0.53442221,\n",
              "        0.55392643, 0.58713816, 0.60889288, 0.62727179, 0.63282983,\n",
              "        0.64479835, 0.65857398, 0.62478268, 0.60043647, 0.57769287,\n",
              "        0.59024112, 0.60752886, 0.60606266, 0.60551708, 0.61100695,\n",
              "        0.60582395, 0.61561024, 0.62386196, 0.61881549, 0.61919052,\n",
              "        0.63102262, 0.62965871, 0.61540566, 0.58870665, 0.58451253,\n",
              "        0.60336887, 0.59808368, 0.56528116, 0.57527193, 0.6001296 ,\n",
              "        0.5799775 , 0.56773624, 0.57534019, 0.59798139, 0.56739523,\n",
              "        0.60295971, 0.60009546, 0.63071575, 0.61202986, 0.63027246,\n",
              "        0.6491288 , 0.66525732, 0.6468783 , 0.68605718, 0.69775297,\n",
              "        0.68783036, 0.65888096, 0.64728746, 0.66188152, 0.68847823,\n",
              "        0.67886249, 0.70112866, 0.71521127, 0.75016199, 0.74112597,\n",
              "        0.76441505, 0.76417633, 0.76226684, 0.75912986, 0.75306037,\n",
              "        0.72939616, 0.7464453 , 0.74085313, 0.7274184 , 0.72881644,\n",
              "        0.72479287, 0.70791419, 0.72363354, 0.73014633, 0.73195353,\n",
              "        0.74399022, 0.82040446, 0.80529884, 0.79677428, 0.7823848 ,\n",
              "        0.77099599, 0.78804513, 0.78483988, 0.74013709, 0.74204659,\n",
              "        0.72847544, 0.73771615, 0.77413308, 0.77249632, 0.75374227,\n",
              "        0.76611996, 0.78323726, 0.77512196, 0.80376448, 0.81453955,\n",
              "        0.81409626, 0.83145226, 0.8177448 , 0.81910871, 0.81917687,\n",
              "        0.80857235, 0.8232687 , 0.84451186, 0.83462338, 0.8229277 ,\n",
              "        0.83046338, 0.82637165, 0.84515973, 0.84509157, 0.82575781,\n",
              "        0.84427325, 0.82091591, 0.82988377, 0.85286597, 0.8581853 ,\n",
              "        0.86701674, 0.86084496, 0.90950326, 0.89671641, 0.88246325,\n",
              "        0.90524098, 0.90629801, 0.8997511 , 0.89194257, 0.87543901,\n",
              "        0.89170396, 0.90104685, 0.89739831, 0.88219051, 0.87138131,\n",
              "        0.86012892, 0.84577358, 0.86183384, 0.87741677, 0.89432949,\n",
              "        0.95775229, 0.96685647, 0.9510008 , 0.96965256, 0.91864157,\n",
              "        0.91175365, 0.89692099, 0.89449994, 0.92068749, 0.93449725,\n",
              "        0.93633858, 0.92985983, 0.92822318, 0.92423363, 0.93340608,\n",
              "        0.93521328, 0.94673852, 0.91908487, 0.90885538, 0.90773008,\n",
              "        0.92222185, 0.935793  , 0.94315824, 0.95297856, 0.93930512,\n",
              "        0.96743619, 0.9948171 , 0.99045253, 0.99955671, 0.97647221,\n",
              "        0.97981388, 1.        , 0.98428075, 0.98574695, 0.98809974,\n",
              "        0.98066634, 0.9807345 , 0.9718349 , 0.96890239, 0.94046445,\n",
              "        0.90970784, 0.91577733, 0.86711903, 0.87649605, 0.90036484,\n",
              "        0.90247892, 0.85838988, 0.85484363, 0.85409357, 0.86643712,\n",
              "        0.80922023, 0.8321683 , 0.83448706, 0.81941558, 0.82224569,\n",
              "        0.80656057, 0.80083198, 0.80345761, 0.81706279, 0.80420778,\n",
              "        0.8402837 , 0.85613938, 0.85883317, 0.862584  , 0.80369622,\n",
              "        0.81760837, 0.77368978, 0.76144851, 0.77747474, 0.80015007,\n",
              "        0.82200708, 0.81552832, 0.82759914, 0.84202275, 0.86002663,\n",
              "        0.85146793, 0.84038599, 0.8140281 , 0.814369  , 0.85920831,\n",
              "        0.8819518 , 0.86626657, 0.85879904, 0.85170665, 0.87424555,\n",
              "        0.85961747, 0.84679649, 0.85978792, 0.83271388, 0.84945615,\n",
              "        0.80318477, 0.75592461, 0.75520858, 0.74310363, 0.78071402,\n",
              "        0.79755857, 0.82398473, 0.8214615 , 0.82122278, 0.83738533,\n",
              "        0.83482796, 0.86183384, 0.83878337, 0.83499851, 0.80656057,\n",
              "        0.83639656, 0.8236097 , 0.8399427 , 0.87724622, 0.87738264,\n",
              "        0.86619841, 0.87104031, 0.84372766, 0.85119519, 0.84434141,\n",
              "        0.80219599, 0.83046338, 0.82817886, 0.8154943 , 0.83697617,\n",
              "        0.83319131, 0.81007268, 0.82855389, 0.78166877, 0.78657892,\n",
              "        0.77624715, 0.73058962, 0.74947999, 0.72029187, 0.70146967,\n",
              "        0.7015038 , 0.7256112 , 0.76499476, 0.78456714, 0.79820645,\n",
              "        0.50755275, 0.50526818, 0.46373651, 0.44760799, 0.48791217,\n",
              "        0.47451157, 0.44545983, 0.43915163, 0.45040407, 0.43519622,\n",
              "        0.40508749, 0.39980225, 0.38589014, 0.37351245, 0.4047124 ,\n",
              "        0.41453267, 0.41640809, 0.39069801, 0.4064514 , 0.38892489,\n",
              "        0.37900228, 0.33607257, 0.34568825, 0.37368296, 0.36246465,\n",
              "        0.33654994, 0.33320832, 0.35162137, 0.39117539, 0.40553073,\n",
              "        0.43502577, 0.41797663, 0.4355713 , 0.42469399, 0.44552804,\n",
              "        0.45320015, 0.45923551, 0.48061514, 0.47376141, 0.45504144,\n",
              "        0.46353192, 0.49435674, 0.48736658, 0.45824668, 0.45705322,\n",
              "        0.45493915, 0.43492348, 0.42701265, 0.42991103, 0.41350971,\n",
              "        0.41552155, 0.4378218 , 0.38022982, 0.33811848, 0.32461554,\n",
              "        0.33443586, 0.31384048, 0.29338151, 0.39833599, 0.38040032,\n",
              "        0.41674909, 0.41981791, 0.45862177, 0.40703106, 0.39165276,\n",
              "        0.36587447, 0.37078458, 0.34040306, 0.34892763, 0.37409212,\n",
              "        0.37893407, 0.38773143, 0.35233746, 0.34909808, 0.35677019,\n",
              "        0.36594263, 0.31496573, 0.3236608 , 0.35025746, 0.36219185,\n",
              "        0.35711119, 0.34006206, 0.3749105 , 0.34735909, 0.35919119,\n",
              "        0.36396493, 0.36734068, 0.32424046, 0.29549564, 0.25693045,\n",
              "        0.25512325, 0.27428651, 0.24537114, 0.25515738, 0.23234563,\n",
              "        0.22825384, 0.23814232, 0.27704846, 0.27476389, 0.24472326,\n",
              "        0.25583933, 0.24666689, 0.2425069 , 0.2703311 , 0.27571863,\n",
              "        0.28397041, 0.27950354, 0.25222493, 0.25355476, 0.25430492,\n",
              "        0.23575545, 0.25843078, 0.26705764, 0.29621168, 0.32113751,\n",
              "        0.3214103 , 0.27401372, 0.26507994, 0.23950623, 0.27507076,\n",
              "        0.24485969, 0.23933578, 0.24216589, 0.24305248, 0.2724111 ,\n",
              "        0.27844651, 0.26664848, 0.27735534, 0.27149043, 0.30494083,\n",
              "        0.3020425 , 0.31230607, 0.3136359 , 0.30879395, 0.29304056,\n",
              "        0.29239269, 0.26954685, 0.25280459, 0.24618952, 0.25352062,\n",
              "        0.27234289, 0.2484741 , 0.23957444, 0.23272071, 0.25239538,\n",
              "        0.26068128, 0.24349578, 0.23742624, 0.24373444, 0.24942884,\n",
              "        0.27360451, 0.27295668, 0.21897911, 0.21331878, 0.20677192,\n",
              "        0.19565585, 0.20155489, 0.1949739 , 0.18143688, 0.1838238 ,\n",
              "        0.1756061 , 0.16183037, 0.15511301, 0.17969789, 0.16196679,\n",
              "        0.15947758, 0.1694684 , 0.17516281, 0.17073002, 0.17103695,\n",
              "        0.15187368, 0.15303301, 0.13513142, 0.13158522, 0.14109862,\n",
              "        0.12906196, 0.15388547, 0.14965732, 0.15112352, 0.14532682,\n",
              "        0.14014387, 0.13915504, 0.16571757, 0.13949605, 0.03079074,\n",
              "        0.0350871 , 0.01449177, 0.02144779, 0.00555801, 0.        ,\n",
              "        0.00641047, 0.02663074, 0.02577829, 0.04282742, 0.07828963]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def GetTrainedModel(x_train, y_train):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(60, return_sequences=True, input_shape=(N_STEPS, len(['scaled_close']))))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(LSTM(120, return_sequences=False))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(20))\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  BATCH_SIZE = 8\n",
        "  EPOCHS = 80\n",
        "\n",
        "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            epochs=EPOCHS,\n",
        "            verbose=1)\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "yCIeiEz7WDbj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GET PREDICTIONS\n",
        "predictions = []\n",
        "\n",
        "for step in LOOKUP_STEPS:\n",
        "  df, last_sequence, x_train, y_train = PrepareData(step)\n",
        "  x_train = x_train[:, :, :len(['scaled_close'])].astype(np.float32)\n",
        "\n",
        "  model = GetTrainedModel(x_train, y_train)\n",
        "\n",
        "  last_sequence = last_sequence[-N_STEPS:]\n",
        "  last_sequence = np.expand_dims(last_sequence, axis=0)\n",
        "  prediction = model.predict(last_sequence)\n",
        "  predicted_price = scaler.inverse_transform(prediction)[0][0]\n",
        "\n",
        "  predictions.append(round(float(predicted_price), 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX9T12C_WGEP",
        "outputId": "2e1196b8-9c2b-4b08-9e6d-062edb7e8384"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "94/94 [==============================] - 4s 9ms/step - loss: 0.0219\n",
            "Epoch 2/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0050\n",
            "Epoch 3/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0043\n",
            "Epoch 4/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0038\n",
            "Epoch 5/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0039\n",
            "Epoch 6/80\n",
            "94/94 [==============================] - 1s 15ms/step - loss: 0.0038\n",
            "Epoch 7/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0034\n",
            "Epoch 8/80\n",
            "94/94 [==============================] - 1s 13ms/step - loss: 0.0030\n",
            "Epoch 9/80\n",
            "94/94 [==============================] - 1s 13ms/step - loss: 0.0028\n",
            "Epoch 10/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0026\n",
            "Epoch 11/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0031\n",
            "Epoch 12/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0024\n",
            "Epoch 13/80\n",
            "94/94 [==============================] - 1s 13ms/step - loss: 0.0023\n",
            "Epoch 14/80\n",
            "94/94 [==============================] - 1s 13ms/step - loss: 0.0020\n",
            "Epoch 15/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0021\n",
            "Epoch 16/80\n",
            "94/94 [==============================] - 1s 14ms/step - loss: 0.0019\n",
            "Epoch 17/80\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0020\n",
            "Epoch 18/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0021\n",
            "Epoch 19/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0018\n",
            "Epoch 20/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0017\n",
            "Epoch 21/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0018\n",
            "Epoch 22/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0018\n",
            "Epoch 23/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0016\n",
            "Epoch 24/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0017\n",
            "Epoch 25/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0017\n",
            "Epoch 26/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0017\n",
            "Epoch 27/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0015\n",
            "Epoch 28/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0016\n",
            "Epoch 29/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0017\n",
            "Epoch 30/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0016\n",
            "Epoch 31/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0016\n",
            "Epoch 32/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0019\n",
            "Epoch 33/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0016\n",
            "Epoch 34/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0016\n",
            "Epoch 35/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0014\n",
            "Epoch 36/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0015\n",
            "Epoch 37/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0014\n",
            "Epoch 38/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0014\n",
            "Epoch 39/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0015\n",
            "Epoch 40/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0015\n",
            "Epoch 41/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0015\n",
            "Epoch 42/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0015\n",
            "Epoch 43/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0017\n",
            "Epoch 44/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0017\n",
            "Epoch 45/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0015\n",
            "Epoch 46/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0016\n",
            "Epoch 47/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0016\n",
            "Epoch 48/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0015\n",
            "Epoch 49/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0015\n",
            "Epoch 50/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0014\n",
            "Epoch 51/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0013\n",
            "Epoch 52/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0014\n",
            "Epoch 53/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0013\n",
            "Epoch 54/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0014\n",
            "Epoch 55/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0015\n",
            "Epoch 56/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0014\n",
            "Epoch 57/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0020\n",
            "Epoch 58/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0015\n",
            "Epoch 59/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0014\n",
            "Epoch 60/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0016\n",
            "Epoch 61/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0015\n",
            "Epoch 62/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0014\n",
            "Epoch 63/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0015\n",
            "Epoch 64/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0015\n",
            "Epoch 65/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0014\n",
            "Epoch 66/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0015\n",
            "Epoch 67/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0015\n",
            "Epoch 68/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0014\n",
            "Epoch 69/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0013\n",
            "Epoch 70/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0015\n",
            "Epoch 71/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0013\n",
            "Epoch 72/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0014\n",
            "Epoch 73/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0014\n",
            "Epoch 74/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0015\n",
            "Epoch 75/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0013\n",
            "Epoch 76/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0014\n",
            "Epoch 77/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0013\n",
            "Epoch 78/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0014\n",
            "Epoch 79/80\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0014\n",
            "Epoch 80/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0015\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 7, 60)             14880     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 7, 60)             0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 120)               86880     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 120)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 20)                2420      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 104,201\n",
            "Trainable params: 104,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 1s 737ms/step\n",
            "Epoch 1/80\n",
            "94/94 [==============================] - 4s 8ms/step - loss: 0.0244\n",
            "Epoch 2/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0054\n",
            "Epoch 3/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0053\n",
            "Epoch 4/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0047\n",
            "Epoch 5/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0042\n",
            "Epoch 6/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0036\n",
            "Epoch 7/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0040\n",
            "Epoch 8/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0035\n",
            "Epoch 9/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0034\n",
            "Epoch 10/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0029\n",
            "Epoch 11/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0034\n",
            "Epoch 12/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0027\n",
            "Epoch 13/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0027\n",
            "Epoch 14/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0028\n",
            "Epoch 15/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0027\n",
            "Epoch 16/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0026\n",
            "Epoch 17/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0025\n",
            "Epoch 18/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0024\n",
            "Epoch 19/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0027\n",
            "Epoch 20/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0023\n",
            "Epoch 21/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0023\n",
            "Epoch 22/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0024\n",
            "Epoch 23/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0024\n",
            "Epoch 24/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0022\n",
            "Epoch 25/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0022\n",
            "Epoch 26/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0024\n",
            "Epoch 27/80\n",
            "94/94 [==============================] - 1s 13ms/step - loss: 0.0019\n",
            "Epoch 28/80\n",
            "94/94 [==============================] - 1s 14ms/step - loss: 0.0022\n",
            "Epoch 29/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0022\n",
            "Epoch 30/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0021\n",
            "Epoch 31/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0022\n",
            "Epoch 32/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0023\n",
            "Epoch 33/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0022\n",
            "Epoch 34/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0022\n",
            "Epoch 35/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0021\n",
            "Epoch 36/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0021\n",
            "Epoch 37/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0023\n",
            "Epoch 38/80\n",
            "94/94 [==============================] - 1s 15ms/step - loss: 0.0021\n",
            "Epoch 39/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0019\n",
            "Epoch 40/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0021\n",
            "Epoch 41/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0022\n",
            "Epoch 42/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0021\n",
            "Epoch 43/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0020\n",
            "Epoch 44/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0021\n",
            "Epoch 45/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0021\n",
            "Epoch 46/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0019\n",
            "Epoch 47/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0020\n",
            "Epoch 48/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0018\n",
            "Epoch 49/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0023\n",
            "Epoch 50/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0020\n",
            "Epoch 51/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0021\n",
            "Epoch 52/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0020\n",
            "Epoch 53/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0023\n",
            "Epoch 54/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0020\n",
            "Epoch 55/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0020\n",
            "Epoch 56/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0021\n",
            "Epoch 57/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0020\n",
            "Epoch 58/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0020\n",
            "Epoch 59/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0021\n",
            "Epoch 60/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0021\n",
            "Epoch 61/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0020\n",
            "Epoch 62/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0020\n",
            "Epoch 63/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0019\n",
            "Epoch 64/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0021\n",
            "Epoch 65/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0021\n",
            "Epoch 66/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0020\n",
            "Epoch 67/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0021\n",
            "Epoch 68/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0018\n",
            "Epoch 69/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0019\n",
            "Epoch 70/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0020\n",
            "Epoch 71/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0019\n",
            "Epoch 72/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0019\n",
            "Epoch 73/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0020\n",
            "Epoch 74/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0022\n",
            "Epoch 75/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0018\n",
            "Epoch 76/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0021\n",
            "Epoch 77/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0020\n",
            "Epoch 78/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0021\n",
            "Epoch 79/80\n",
            "94/94 [==============================] - 1s 13ms/step - loss: 0.0019\n",
            "Epoch 80/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0017\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2 (LSTM)               (None, 7, 60)             14880     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 7, 60)             0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 120)               86880     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 120)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 20)                2420      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 104,201\n",
            "Trainable params: 104,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 1s 684ms/step\n",
            "Epoch 1/80\n",
            "94/94 [==============================] - 4s 9ms/step - loss: 0.0305\n",
            "Epoch 2/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0059\n",
            "Epoch 3/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0054\n",
            "Epoch 4/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0051\n",
            "Epoch 5/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0050\n",
            "Epoch 6/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0044\n",
            "Epoch 7/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0042\n",
            "Epoch 8/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0036\n",
            "Epoch 9/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0039\n",
            "Epoch 10/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0047\n",
            "Epoch 11/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0042\n",
            "Epoch 12/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0046\n",
            "Epoch 13/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0036\n",
            "Epoch 14/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0035\n",
            "Epoch 15/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0032\n",
            "Epoch 16/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0036\n",
            "Epoch 17/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0034\n",
            "Epoch 18/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0029\n",
            "Epoch 19/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0031\n",
            "Epoch 20/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0031\n",
            "Epoch 21/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0029\n",
            "Epoch 22/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0028\n",
            "Epoch 23/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0028\n",
            "Epoch 24/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0027\n",
            "Epoch 25/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0034\n",
            "Epoch 26/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0026\n",
            "Epoch 27/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0027\n",
            "Epoch 28/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0026\n",
            "Epoch 29/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0027\n",
            "Epoch 30/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0028\n",
            "Epoch 31/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0026\n",
            "Epoch 32/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0029\n",
            "Epoch 33/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0029\n",
            "Epoch 34/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0030\n",
            "Epoch 35/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0025\n",
            "Epoch 36/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0026\n",
            "Epoch 37/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0026\n",
            "Epoch 38/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0027\n",
            "Epoch 39/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0027\n",
            "Epoch 40/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0025\n",
            "Epoch 41/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0025\n",
            "Epoch 42/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0026\n",
            "Epoch 43/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0026\n",
            "Epoch 44/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0026\n",
            "Epoch 45/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0026\n",
            "Epoch 46/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0027\n",
            "Epoch 47/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0027\n",
            "Epoch 48/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0023\n",
            "Epoch 49/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0026\n",
            "Epoch 50/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0027\n",
            "Epoch 51/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0026\n",
            "Epoch 52/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0027\n",
            "Epoch 53/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0027\n",
            "Epoch 54/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0026\n",
            "Epoch 55/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0029\n",
            "Epoch 56/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0026\n",
            "Epoch 57/80\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 0.0024\n",
            "Epoch 58/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0025\n",
            "Epoch 59/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0024\n",
            "Epoch 60/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0025\n",
            "Epoch 61/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0025\n",
            "Epoch 62/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0026\n",
            "Epoch 63/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0026\n",
            "Epoch 64/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0026\n",
            "Epoch 65/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0024\n",
            "Epoch 66/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0027\n",
            "Epoch 67/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0025\n",
            "Epoch 68/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0024\n",
            "Epoch 69/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0023\n",
            "Epoch 70/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0027\n",
            "Epoch 71/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0022\n",
            "Epoch 72/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0023\n",
            "Epoch 73/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0024\n",
            "Epoch 74/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0024\n",
            "Epoch 75/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0027\n",
            "Epoch 76/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0024\n",
            "Epoch 77/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0024\n",
            "Epoch 78/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0026\n",
            "Epoch 79/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0023\n",
            "Epoch 80/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0027\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_4 (LSTM)               (None, 7, 60)             14880     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 7, 60)             0         \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 120)               86880     \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 120)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 20)                2420      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 104,201\n",
            "Trainable params: 104,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 1s 610ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if bool(predictions) == True and len(predictions) > 0:\n",
        "  predictions_list = [str(d)+'$' for d in predictions]\n",
        "  predictions_str = ', '.join(predictions_list)\n",
        "  message = f' prediction for upcoming 3 days ({predictions_str})'\n",
        "  \n",
        "  print(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC065byaWJJS",
        "outputId": "60f29b07-7090-4b6f-dda5-12e12f80a029"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " prediction for upcoming 3 days (106.12$, 97.12$, 87.81$)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "today=start_date =datetime.datetime.today()\n",
        "day1=(today+datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "day2=(today+datetime.timedelta(days=2)).strftime('%Y-%m-%d')\n",
        "day3=(today+datetime.timedelta(days=3)).strftime('%Y-%m-%d')\n",
        "days=[day1,day2,day3]\n",
        "meta_lstm=pd.DataFrame()\n",
        "meta_lstm['future days']=days\n",
        "meta_lstm['price']=predictions\n",
        "meta_lstm['ticker']='META'\n",
        "\n",
        "meta_lstm.to_csv('meta_lstm.csv') \n",
        "files.download('meta_lstm.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "D29dx0tOD6i4",
        "outputId": "94ce629b-3a6b-453f-ec52-f25660f40dd3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_50d1d394-91fa-4204-9448-df40374b0c70\", \"meta_lstm.csv\", 99)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
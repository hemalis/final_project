{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance\n",
        "!pip install yahoofinancials"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPNqJ62FFawm",
        "outputId": "58837761-40d6-4058-b3cf-6ad4dbcec7e2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.85-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from yfinance) (4.9.1)\n",
            "Collecting requests>=2.26\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Installing collected packages: requests, yfinance\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed requests-2.28.1 yfinance-0.1.85\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yahoofinancials\n",
            "  Downloading yahoofinancials-1.6.tar.gz (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from yahoofinancials) (4.6.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from yahoofinancials) (2022.6)\n",
            "Building wheels for collected packages: yahoofinancials\n",
            "  Building wheel for yahoofinancials (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yahoofinancials: filename=yahoofinancials-1.6-py3-none-any.whl size=15191 sha256=83e880a16ead2c1cca671a93393b0f9d4bdf65ecaad519ab128b54b2198568a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/63/46/e7110bfee88685fe69e338d1b14d1748921862aa57b6705b60\n",
            "Successfully built yahoofinancials\n",
            "Installing collected packages: yahoofinancials\n",
            "Successfully installed yahoofinancials-1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aEHgGMtF8NLd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time as tm\n",
        "import datetime \n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "# Data preparation\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from collections import deque\n",
        "\n",
        "# AI\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout\n",
        "\n",
        "# Graphics library\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "from yahoofinancials import YahooFinancials\n",
        "import time\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wt9C0HKdAyrU"
      },
      "outputs": [],
      "source": [
        "#time to pull the data\n",
        "start_date =datetime.datetime.today().strftime('%Y-%m-%d')\n",
        "end_date = (datetime.datetime.today() - datetime.timedelta(days=1100)).strftime('%Y-%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calling API \n",
        "data=YahooFinancials('goog').get_historical_price_data(end_date,start_date,\"daily\")\n",
        "#into the dataframe\n",
        "goog_df = pd.DataFrame(data['GOOG']['prices'])\n",
        "goog_df =goog_df.drop(['high','low','open','volume','date','adjclose'], axis=1)\n",
        "goog_df.rename(columns={'formatted_date':'date'},inplace=True)\n",
        "goog_df['date']=pd.to_datetime(goog_df['date'])"
      ],
      "metadata": {
        "id": "b7r-ENorFm9p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9VtXlmLY-7h"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "r-Mb0wsxXipU"
      },
      "outputs": [],
      "source": [
        "# SETTINGS\n",
        "\n",
        "# Window size or the sequence length, 7 (1 week)\n",
        "N_STEPS = 7\n",
        "\n",
        "# Lookup steps, 1 is the next day, 3 = after tomorrow\n",
        "LOOKUP_STEPS = [1, 2, 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FRC5O4aUckB3"
      },
      "outputs": [],
      "source": [
        "# Scale data for ML engine\n",
        "scaler = MinMaxScaler()\n",
        "goog_df['scaled_close'] = scaler.fit_transform(np.expand_dims(goog_df['close'].values, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3ww7vT6zGX4k"
      },
      "outputs": [],
      "source": [
        "def PrepareData(days):\n",
        "    df = goog_df.copy()\n",
        "    df['future'] = df['scaled_close'].shift(-days)\n",
        "    last_sequence = np.array(df[['scaled_close']].tail(days))\n",
        "    df.dropna(inplace=True)\n",
        "    sequence_data = []\n",
        "    sequences = deque(maxlen=N_STEPS)\n",
        "    \n",
        "    for entry, target in zip(df[['scaled_close'] + ['date']].values, df['future'].values):\n",
        "        sequences.append(entry)\n",
        "        if len(sequences) == N_STEPS:\n",
        "            sequence_data.append([np.array(sequences), target])\n",
        "            \n",
        "    last_sequence = list([s[:len(['scaled_close'])] for s in sequences]) + list(last_sequence)\n",
        "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
        "\n",
        "    # construct the X's and Y's\n",
        "    X, Y = [], []\n",
        "    for seq, target in sequence_data:\n",
        "        X.append(seq)\n",
        "        Y.append(target)\n",
        "\n",
        "   # convert to numpy arrays\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    return df, last_sequence, X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBnVZJ47W32T",
        "outputId": "15dae1d3-38e9-4009-8ec4-ff56948ee712"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(         close       date  scaled_close    future\n",
              " 0    65.443001 2019-11-07      0.128854  0.123715\n",
              " 1    65.568497 2019-11-08      0.130136  0.123307\n",
              " 2    64.959503 2019-11-11      0.123914  0.130182\n",
              " 3    64.940002 2019-11-12      0.123715  0.142141\n",
              " 4    64.900002 2019-11-13      0.123307  0.134903\n",
              " ..         ...        ...           ...       ...\n",
              " 751  90.500000 2022-11-01      0.384857  0.346033\n",
              " 752  87.070000 2022-11-02      0.349813  0.365956\n",
              " 753  83.489998 2022-11-03      0.313237  0.368612\n",
              " 754  86.699997 2022-11-04      0.346033  0.353185\n",
              " 755  88.650002 2022-11-07      0.365956  0.422352\n",
              " \n",
              " [756 rows x 4 columns], array([[0.4469748 ],\n",
              "        [0.42735857],\n",
              "        [0.38485664],\n",
              "        [0.349813  ],\n",
              "        [0.31323686],\n",
              "        [0.34603277],\n",
              "        [0.3659556 ],\n",
              "        [0.36861196],\n",
              "        [0.35318458],\n",
              "        [0.42235228]], dtype=float32), array([[[0.12885428517199038, Timestamp('2019-11-07 00:00:00')],\n",
              "         [0.13013645187704637, Timestamp('2019-11-08 00:00:00')],\n",
              "         [0.12391448636358149, Timestamp('2019-11-11 00:00:00')],\n",
              "         ...,\n",
              "         [0.12330656988642585, Timestamp('2019-11-13 00:00:00')],\n",
              "         [0.1301824411989494, Timestamp('2019-11-14 00:00:00')],\n",
              "         [0.1421412238538039, Timestamp('2019-11-15 00:00:00')]],\n",
              " \n",
              "        [[0.13013645187704637, Timestamp('2019-11-08 00:00:00')],\n",
              "         [0.12391448636358149, Timestamp('2019-11-11 00:00:00')],\n",
              "         [0.12371525126733707, Timestamp('2019-11-12 00:00:00')],\n",
              "         ...,\n",
              "         [0.1301824411989494, Timestamp('2019-11-14 00:00:00')],\n",
              "         [0.1421412238538039, Timestamp('2019-11-15 00:00:00')],\n",
              "         [0.13490266048227206, Timestamp('2019-11-18 00:00:00')]],\n",
              " \n",
              "        [[0.12391448636358149, Timestamp('2019-11-11 00:00:00')],\n",
              "         [0.12371525126733707, Timestamp('2019-11-12 00:00:00')],\n",
              "         [0.12330656988642585, Timestamp('2019-11-13 00:00:00')],\n",
              "         ...,\n",
              "         [0.1421412238538039, Timestamp('2019-11-15 00:00:00')],\n",
              "         [0.13490266048227206, Timestamp('2019-11-18 00:00:00')],\n",
              "         [0.13222584810350557, Timestamp('2019-11-19 00:00:00')]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0.4289932274877398, Timestamp('2022-10-26 00:00:00')],\n",
              "         [0.40631191750918716, Timestamp('2022-10-27 00:00:00')],\n",
              "         [0.44697481850781806, Timestamp('2022-10-28 00:00:00')],\n",
              "         ...,\n",
              "         [0.38485665167336824, Timestamp('2022-11-01 00:00:00')],\n",
              "         [0.34981302222726285, Timestamp('2022-11-02 00:00:00')],\n",
              "         [0.31323685708974114, Timestamp('2022-11-03 00:00:00')]],\n",
              " \n",
              "        [[0.40631191750918716, Timestamp('2022-10-27 00:00:00')],\n",
              "         [0.44697481850781806, Timestamp('2022-10-28 00:00:00')],\n",
              "         [0.427358579912098, Timestamp('2022-10-31 00:00:00')],\n",
              "         ...,\n",
              "         [0.34981302222726285, Timestamp('2022-11-02 00:00:00')],\n",
              "         [0.31323685708974114, Timestamp('2022-11-03 00:00:00')],\n",
              "         [0.34603277791483633, Timestamp('2022-11-04 00:00:00')]],\n",
              " \n",
              "        [[0.44697481850781806, Timestamp('2022-10-28 00:00:00')],\n",
              "         [0.427358579912098, Timestamp('2022-10-31 00:00:00')],\n",
              "         [0.38485665167336824, Timestamp('2022-11-01 00:00:00')],\n",
              "         ...,\n",
              "         [0.31323685708974114, Timestamp('2022-11-03 00:00:00')],\n",
              "         [0.34603277791483633, Timestamp('2022-11-04 00:00:00')],\n",
              "         [0.3659555860072422, Timestamp('2022-11-07 00:00:00')]]],\n",
              "       dtype=object), array([0.12588626, 0.12501784, 0.1219477 , 0.12774579, 0.13125009,\n",
              "        0.13096403, 0.12686201, 0.11917899, 0.12191707, 0.13482089,\n",
              "        0.13869818, 0.14507854, 0.14658044, 0.14714237, 0.14732625,\n",
              "        0.15000813, 0.14876174, 0.15557634, 0.1524857 , 0.1512086 ,\n",
              "        0.15295573, 0.14966079, 0.14927768, 0.14658044, 0.15518294,\n",
              "        0.1508357 , 0.14278999, 0.14323951, 0.15874352, 0.15531576,\n",
              "        0.1724545 , 0.17201004, 0.1776191 , 0.1855422 , 0.19059955,\n",
              "        0.19545251, 0.19118696, 0.19543716, 0.20182266, 0.21647868,\n",
              "        0.21852715, 0.21931894, 0.21967649, 0.2094904 , 0.19272971,\n",
              "        0.20226197, 0.20536282, 0.20393754, 0.19289831, 0.2193138 ,\n",
              "        0.19945748, 0.20005004, 0.21435358, 0.21588611, 0.23093031,\n",
              "        0.23098651, 0.23582927, 0.23398517, 0.23709109, 0.23654444,\n",
              "        0.24013059, 0.235768  , 0.21888984, 0.18644126, 0.16951204,\n",
              "        0.17192827, 0.13356936, 0.14441957, 0.16984917, 0.14547194,\n",
              "        0.16852608, 0.13405466, 0.12351602, 0.0811929 , 0.11431067,\n",
              "        0.08111628, 0.02977684, 0.0833231 , 0.01415536, 0.03227488,\n",
              "        0.02052554, 0.02997097, 0.00802019, 0.        , 0.03976377,\n",
              "        0.02343222, 0.05370462, 0.02763132, 0.04607775, 0.05424608,\n",
              "        0.02503113, 0.03280613, 0.02107726, 0.06656245, 0.066353  ,\n",
              "        0.07849567, 0.07909333, 0.08221457, 0.10860967, 0.10515642,\n",
              "        0.10566725, 0.11577165, 0.1072713 , 0.08159137, 0.10553443,\n",
              "        0.11222642, 0.11375895, 0.11200676, 0.0904442 , 0.14551785,\n",
              "        0.1491857 , 0.13485667, 0.13801871, 0.15043723, 0.14849095,\n",
              "        0.16139477, 0.1694712 , 0.1770776 , 0.16301928, 0.14952797,\n",
              "        0.15300172, 0.16171662, 0.16720813, 0.16186729, 0.17884507,\n",
              "        0.17684258, 0.18073523, 0.18410671, 0.18452561, 0.18395861,\n",
              "        0.19018572, 0.1916672 , 0.19544737, 0.1939966 , 0.18163428,\n",
              "        0.19502341, 0.19922247, 0.204101  , 0.20905109, 0.17737388,\n",
              "        0.18214507, 0.18555242, 0.19723534, 0.20152638, 0.19378201,\n",
              "        0.19161607, 0.20190443, 0.20831542, 0.19174383, 0.19652523,\n",
              "        0.15492758, 0.17284268, 0.18236473, 0.1948446 , 0.20846359,\n",
              "        0.22429967, 0.21892562, 0.22445291, 0.23211037, 0.24781868,\n",
              "        0.23228918, 0.23700932, 0.23346409, 0.23569138, 0.23443977,\n",
              "        0.26006868, 0.25633949, 0.26148367, 0.23450618, 0.23255989,\n",
              "        0.24192363, 0.22666991, 0.23774492, 0.24256218, 0.21779155,\n",
              "        0.21344432, 0.20860148, 0.21301513, 0.2265473 , 0.22368154,\n",
              "        0.22450397, 0.2164429 , 0.22987802, 0.23592125, 0.23044501,\n",
              "        0.23568117, 0.25643147, 0.2507765 , 0.26825743, 0.26757803,\n",
              "        0.27155237, 0.28177938, 0.30433808, 0.29511738, 0.30026662,\n",
              "        0.29504075, 0.30859334, 0.34311082, 0.29895382, 0.27300314,\n",
              "        0.24304234, 0.25559369, 0.24285332, 0.23708088, 0.23634521,\n",
              "        0.24766544, 0.23717278, 0.22421275, 0.2060575 , 0.19133   ,\n",
              "        0.20885185, 0.18318209, 0.18986388, 0.19837962, 0.20837162,\n",
              "        0.21082877, 0.21096674, 0.22143383, 0.20525549, 0.21935472,\n",
              "        0.2027115 , 0.20621082, 0.21930873, 0.23427124, 0.2618208 ,\n",
              "        0.26311325, 0.26127422, 0.25670218, 0.26379265, 0.24417641,\n",
              "        0.25506754, 0.27416269, 0.28541145, 0.29852472, 0.27270171,\n",
              "        0.2797564 , 0.23498642, 0.26084512, 0.28831298, 0.29087739,\n",
              "        0.3032295 , 0.35376179, 0.36103621, 0.36020864, 0.36084719,\n",
              "        0.34929708, 0.35559061, 0.35412448, 0.36800913, 0.37023641,\n",
              "        0.36449967, 0.35256131, 0.36131713, 0.35021656, 0.34647209,\n",
              "        0.36385091, 0.36515358, 0.37626943, 0.3596927 , 0.37877764,\n",
              "        0.39402622, 0.39342345, 0.39404664, 0.38969941, 0.38922431,\n",
              "        0.37164119, 0.36714585, 0.37043565, 0.35934528, 0.36328384,\n",
              "        0.36084719, 0.35313345, 0.34450539, 0.348776  , 0.34066902,\n",
              "        0.34520528, 0.34851036, 0.36753403, 0.35866074, 0.34885262,\n",
              "        0.35516665, 0.3430904 , 0.3495678 , 0.34669183, 0.37323507,\n",
              "        0.38343137, 0.36274748, 0.35244385, 0.35645396, 0.34918983,\n",
              "        0.34715157, 0.37507917, 0.42414026, 0.4263624 , 0.43136861,\n",
              "        0.43052576, 0.43963913, 0.39547699, 0.41198739, 0.3980057 ,\n",
              "        0.43152186, 0.4448855 , 0.51771084, 0.51377735, 0.5319786 ,\n",
              "        0.52937841, 0.5245765 , 0.53064015, 0.53090073, 0.53509979,\n",
              "        0.54418767, 0.54746211, 0.54178672, 0.53358261, 0.51505959,\n",
              "        0.51811438, 0.5305329 , 0.49793621, 0.50074585, 0.52355483,\n",
              "        0.52065836, 0.49556082, 0.50699338, 0.53736286, 0.4942633 ,\n",
              "        0.50883755, 0.51002782, 0.54054539, 0.51354748, 0.51588202,\n",
              "        0.52917917, 0.52844358, 0.50041885, 0.50398459, 0.50162954,\n",
              "        0.50897037, 0.50493469, 0.50457715, 0.50007658, 0.51049776,\n",
              "        0.51028832, 0.51697524, 0.55228444, 0.59713619, 0.59672759,\n",
              "        0.60946281, 0.61751367, 0.62795519, 0.61207321, 0.6184485 ,\n",
              "        0.6120987 , 0.63346206, 0.63402399, 0.63639431, 0.63191425,\n",
              "        0.6317405 , 0.61878056, 0.64298411, 0.64882811, 0.63880548,\n",
              "        0.67598948, 0.70152127, 0.69142194, 0.68378491, 0.66288136,\n",
              "        0.66415331, 0.67672508, 0.68558309, 0.65644987, 0.63964326,\n",
              "        0.60404792, 0.61574105, 0.64342343, 0.64610539, 0.63692046,\n",
              "        0.6396177 , 0.66382125, 0.65820713, 0.68965961, 0.69088558,\n",
              "        0.70338072, 0.68753452, 0.69215761, 0.70148043, 0.69712298,\n",
              "        0.68860724, 0.71269333, 0.72001366, 0.72857539, 0.73294305,\n",
              "        0.74837044, 0.7444523 , 0.75114936, 0.7478902 , 0.7444523 ,\n",
              "        0.75134353, 0.74313428, 0.75220174, 0.75776473, 0.75226815,\n",
              "        0.76065099, 0.75771882, 0.75592578, 0.7477421 , 0.7405648 ,\n",
              "        0.75131796, 0.7753325 , 0.78608059, 0.78921207, 0.78001187,\n",
              "        0.78407296, 0.79418251, 0.79858096, 0.80969681, 0.8013598 ,\n",
              "        0.80727543, 0.78079852, 0.7996741 , 0.81498901, 0.82242697,\n",
              "        0.86827473, 0.88695622, 0.8578587 , 0.85361879, 0.85524323,\n",
              "        0.84176212, 0.84961383, 0.85258177, 0.8500123 , 0.8593249 ,\n",
              "        0.86030564, 0.87017511, 0.87114057, 0.86698236, 0.87413409,\n",
              "        0.87430276, 0.87951328, 0.8630081 , 0.85554473, 0.85905411,\n",
              "        0.87461939, 0.90182168, 0.91509327, 0.92072782, 0.91227856,\n",
              "        0.9370799 , 0.94646905, 0.94639251, 0.95027479, 0.93369291,\n",
              "        0.93937345, 0.94697478, 0.94048202, 0.94078851, 0.91021481,\n",
              "        0.92598946, 0.92538677, 0.94377688, 0.93527151, 0.90554058,\n",
              "        0.88054515, 0.88697664, 0.90017683, 0.90924935, 0.91748908,\n",
              "        0.90592378, 0.85160103, 0.83461039, 0.82178327, 0.85444629,\n",
              "        0.82688653, 0.85152948, 0.86355467, 0.88226671, 0.89116042,\n",
              "        0.87881346, 0.85700564, 0.86913294, 0.90501443, 0.90770146,\n",
              "        0.92083507, 0.92963696, 0.91526179, 0.91899612, 0.87654018,\n",
              "        0.87805222, 0.88723714, 0.95625683, 0.95320703, 0.97508626,\n",
              "        0.92914651, 0.95048945, 0.95996029, 0.97930075, 0.98500171,\n",
              "        0.98613071, 0.98507841, 0.95828488, 0.95953127, 0.98913436,\n",
              "        0.98650362, 0.98331601, 0.9831729 , 1.        , 0.99227099,\n",
              "        0.96290797, 0.95962325, 0.95921963, 0.91925663, 0.95305379,\n",
              "        0.91563984, 0.90711903, 0.92917208, 0.91633966, 0.9293763 ,\n",
              "        0.97269559, 0.97968394, 0.9734057 , 0.97921906, 0.95908681,\n",
              "        0.94137094, 0.96587078, 0.94002229, 0.91922591, 0.91512398,\n",
              "        0.93370834, 0.96158489, 0.9635618 , 0.97297652, 0.9564662 ,\n",
              "        0.95704348, 0.95191466, 0.93839785, 0.94243337, 0.93571083,\n",
              "        0.8666146 , 0.86556729, 0.85998387, 0.87601918, 0.8907671 ,\n",
              "        0.90742552, 0.88170985, 0.88840699, 0.85268903, 0.84616556,\n",
              "        0.82424549, 0.78936018, 0.79222087, 0.7550675 , 0.78065557,\n",
              "        0.77943973, 0.8220285 , 0.84664073, 0.86891328, 0.97269559,\n",
              "        0.91766789, 0.92140222, 0.87973808, 0.88254764, 0.90543333,\n",
              "        0.87631024, 0.83061572, 0.84256935, 0.8540684 , 0.8649186 ,\n",
              "        0.81200579, 0.79319662, 0.7823157 , 0.7637467 , 0.81573482,\n",
              "        0.83459512, 0.83839072, 0.8310039 , 0.83696536, 0.83243424,\n",
              "        0.81010027, 0.75229878, 0.76061529, 0.8279184 , 0.81582181,\n",
              "        0.79327831, 0.7551237 , 0.78495175, 0.82612544, 0.83542261,\n",
              "        0.85790983, 0.85460983, 0.89342341, 0.87529878, 0.90399269,\n",
              "        0.90613314, 0.91051102, 0.92379289, 0.91760662, 0.88700735,\n",
              "        0.89774001, 0.927803  , 0.90144878, 0.86173599, 0.85447186,\n",
              "        0.82939474, 0.78634109, 0.77181283, 0.79134224, 0.76035471,\n",
              "        0.76758821, 0.79384546, 0.77049489, 0.73669773, 0.68230857,\n",
              "        0.71945687, 0.68120514, 0.63537771, 0.68023967, 0.634826  ,\n",
              "        0.65720588, 0.66714176, 0.7125605 , 0.65301189, 0.64191139,\n",
              "        0.61559287, 0.63092322, 0.624553  , 0.6163796 , 0.65065186,\n",
              "        0.63304832, 0.65255215, 0.60861481, 0.59170088, 0.57706536,\n",
              "        0.60111053, 0.54246104, 0.54157727, 0.56667481, 0.61268113,\n",
              "        0.62534994, 0.62635118, 0.66322363, 0.63071377, 0.65570913,\n",
              "        0.65794662, 0.65803346, 0.63433048, 0.59866873, 0.55217212,\n",
              "        0.55541592, 0.58807396, 0.54971496, 0.56227652, 0.60467111,\n",
              "        0.6048652 , 0.61151128, 0.67131533, 0.651745  , 0.6103568 ,\n",
              "        0.60713848, 0.5776732 , 0.57469504, 0.62379698, 0.63734957,\n",
              "        0.67916181, 0.68797383, 0.65072334, 0.63363067, 0.60642837,\n",
              "        0.59879648, 0.61235413, 0.58316479, 0.63128591, 0.6321032 ,\n",
              "        0.63557695, 0.56732872, 0.56579618, 0.53749568, 0.62086473,\n",
              "        0.63097934, 0.65192382, 0.64007237, 0.6443634 , 0.67378776,\n",
              "        0.67470731, 0.66806638, 0.66724902, 0.66071027, 0.69289321,\n",
              "        0.68441324, 0.71332681, 0.71567663, 0.71189646, 0.68952164,\n",
              "        0.69503873, 0.66704471, 0.63588344, 0.63281837, 0.6321032 ,\n",
              "        0.6627536 , 0.59736614, 0.58755794, 0.58316479, 0.5754    ,\n",
              "        0.58970354, 0.57059809, 0.55833796, 0.58898836, 0.5781585 ,\n",
              "        0.60227016, 0.60318971, 0.53616744, 0.54188891, 0.5217618 ,\n",
              "        0.51900322, 0.52125093, 0.50061302, 0.48201845, 0.48773983,\n",
              "        0.47343629, 0.46975824, 0.46240213, 0.48947667, 0.46240213,\n",
              "        0.44258159, 0.47476453, 0.50653879, 0.50459757, 0.50480187,\n",
              "        0.47752303, 0.46873658, 0.46199353, 0.46454773, 0.47895338,\n",
              "        0.45310488, 0.48988535, 0.4961176 , 0.48487914, 0.48733115,\n",
              "        0.49703716, 0.51226017, 0.53228509, 0.42899323, 0.40631192,\n",
              "        0.44697482, 0.42735858, 0.38485665, 0.34981302, 0.31323686,\n",
              "        0.34603278, 0.36595559, 0.36861198, 0.35318459, 0.42235229]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "PrepareData(3) # 3 days\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yCIeiEz7WDbj"
      },
      "outputs": [],
      "source": [
        "def GetTrainedModel(x_train, y_train):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(60, return_sequences=True, input_shape=(N_STEPS, len(['scaled_close']))))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(LSTM(120, return_sequences=False))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(20))\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  BATCH_SIZE = 8\n",
        "  EPOCHS = 80\n",
        "\n",
        "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            epochs=EPOCHS,\n",
        "            verbose=1)\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX9T12C_WGEP",
        "outputId": "43d8f6e6-570b-4c7b-ada1-77872dcb86ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "94/94 [==============================] - 4s 10ms/step - loss: 0.0210\n",
            "Epoch 2/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0051\n",
            "Epoch 3/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0043\n",
            "Epoch 4/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0032\n",
            "Epoch 5/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0034\n",
            "Epoch 6/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0030\n",
            "Epoch 7/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0030\n",
            "Epoch 8/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0028\n",
            "Epoch 9/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0026\n",
            "Epoch 10/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0025\n",
            "Epoch 11/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0022\n",
            "Epoch 12/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0023\n",
            "Epoch 13/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0020\n",
            "Epoch 14/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0022\n",
            "Epoch 15/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0020\n",
            "Epoch 16/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0022\n",
            "Epoch 17/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0018\n",
            "Epoch 18/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0019\n",
            "Epoch 19/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0019\n",
            "Epoch 20/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0018\n",
            "Epoch 21/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0021\n",
            "Epoch 22/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0019\n",
            "Epoch 23/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0016\n",
            "Epoch 24/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0017\n",
            "Epoch 25/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0018\n",
            "Epoch 26/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0018\n",
            "Epoch 27/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0018\n",
            "Epoch 28/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0018\n",
            "Epoch 29/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0017\n",
            "Epoch 30/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0015\n",
            "Epoch 31/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0018\n",
            "Epoch 32/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0019\n",
            "Epoch 33/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0016\n",
            "Epoch 34/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0015\n",
            "Epoch 35/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0016\n",
            "Epoch 36/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0015\n",
            "Epoch 37/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0014\n",
            "Epoch 38/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0016\n",
            "Epoch 39/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0017\n",
            "Epoch 40/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0018\n",
            "Epoch 41/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0016\n",
            "Epoch 42/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0016\n",
            "Epoch 43/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0016\n",
            "Epoch 44/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0014\n",
            "Epoch 45/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0014\n",
            "Epoch 46/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0015\n",
            "Epoch 47/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0019\n",
            "Epoch 48/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0014\n",
            "Epoch 49/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0016\n",
            "Epoch 50/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0015\n",
            "Epoch 51/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0015\n",
            "Epoch 52/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0017\n",
            "Epoch 53/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0016\n",
            "Epoch 54/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0015\n",
            "Epoch 55/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0014\n",
            "Epoch 56/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0015\n",
            "Epoch 57/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0014\n",
            "Epoch 58/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0016\n",
            "Epoch 59/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0014\n",
            "Epoch 60/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0017\n",
            "Epoch 61/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0016\n",
            "Epoch 62/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0018\n",
            "Epoch 63/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0018\n",
            "Epoch 64/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0015\n",
            "Epoch 65/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0015\n",
            "Epoch 66/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0016\n",
            "Epoch 67/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0017\n",
            "Epoch 68/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0015\n",
            "Epoch 69/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0014\n",
            "Epoch 70/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0015\n",
            "Epoch 71/80\n",
            "94/94 [==============================] - 1s 10ms/step - loss: 0.0017\n",
            "Epoch 72/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0016\n",
            "Epoch 73/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0014\n",
            "Epoch 74/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0014\n",
            "Epoch 75/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0016\n",
            "Epoch 76/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0014\n",
            "Epoch 77/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0016\n",
            "Epoch 78/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0014\n",
            "Epoch 79/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0016\n",
            "Epoch 80/80\n",
            "94/94 [==============================] - 1s 9ms/step - loss: 0.0016\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 7, 60)             14880     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 7, 60)             0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 120)               86880     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 120)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 20)                2420      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 104,201\n",
            "Trainable params: 104,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 1s 794ms/step\n",
            "Epoch 1/80\n",
            "94/94 [==============================] - 4s 11ms/step - loss: 0.0266\n",
            "Epoch 2/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0050\n",
            "Epoch 3/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0044\n",
            "Epoch 4/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0044\n",
            "Epoch 5/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0034\n",
            "Epoch 6/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0036\n",
            "Epoch 7/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0036\n",
            "Epoch 8/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0036\n",
            "Epoch 9/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0034\n",
            "Epoch 10/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0032\n",
            "Epoch 11/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0030\n",
            "Epoch 12/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0030\n",
            "Epoch 13/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0027\n",
            "Epoch 14/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0025\n",
            "Epoch 15/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0025\n",
            "Epoch 16/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0024\n",
            "Epoch 17/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0025\n",
            "Epoch 18/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0022\n",
            "Epoch 19/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0023\n",
            "Epoch 20/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0021\n",
            "Epoch 21/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0020\n",
            "Epoch 22/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0021\n",
            "Epoch 23/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0028\n",
            "Epoch 24/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0020\n",
            "Epoch 25/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0020\n",
            "Epoch 26/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0021\n",
            "Epoch 27/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0020\n",
            "Epoch 28/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0022\n",
            "Epoch 29/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0021\n",
            "Epoch 30/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0025\n",
            "Epoch 31/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0021\n",
            "Epoch 32/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0020\n",
            "Epoch 33/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0022\n",
            "Epoch 34/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0019\n",
            "Epoch 35/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0020\n",
            "Epoch 36/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0021\n",
            "Epoch 37/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0022\n",
            "Epoch 38/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0022\n",
            "Epoch 39/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0021\n",
            "Epoch 40/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0019\n",
            "Epoch 41/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0021\n",
            "Epoch 42/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0020\n",
            "Epoch 43/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0021\n",
            "Epoch 44/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0021\n",
            "Epoch 45/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0018\n",
            "Epoch 46/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0022\n",
            "Epoch 47/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0024\n",
            "Epoch 48/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0021\n",
            "Epoch 49/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0021\n",
            "Epoch 50/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0021\n",
            "Epoch 51/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0019\n",
            "Epoch 52/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0019\n",
            "Epoch 53/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0018\n",
            "Epoch 54/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0018\n",
            "Epoch 55/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0020\n",
            "Epoch 56/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0021\n",
            "Epoch 57/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0020\n",
            "Epoch 58/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0019\n",
            "Epoch 59/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0021\n",
            "Epoch 60/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0018\n",
            "Epoch 61/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0020\n",
            "Epoch 62/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0019\n",
            "Epoch 63/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0020\n",
            "Epoch 64/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0021\n",
            "Epoch 65/80\n",
            "94/94 [==============================] - 2s 16ms/step - loss: 0.0020\n",
            "Epoch 66/80\n",
            "94/94 [==============================] - 2s 17ms/step - loss: 0.0018\n",
            "Epoch 67/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0019\n",
            "Epoch 68/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0019\n",
            "Epoch 69/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0017\n",
            "Epoch 70/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0021\n",
            "Epoch 71/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0018\n",
            "Epoch 72/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0018\n",
            "Epoch 73/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0019\n",
            "Epoch 74/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0018\n",
            "Epoch 75/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0022\n",
            "Epoch 76/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0021\n",
            "Epoch 77/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0020\n",
            "Epoch 78/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0018\n",
            "Epoch 79/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0018\n",
            "Epoch 80/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0021\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2 (LSTM)               (None, 7, 60)             14880     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 7, 60)             0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 120)               86880     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 120)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 20)                2420      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 104,201\n",
            "Trainable params: 104,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 1s 750ms/step\n",
            "Epoch 1/80\n",
            "94/94 [==============================] - 5s 11ms/step - loss: 0.0231\n",
            "Epoch 2/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0047\n",
            "Epoch 3/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0048\n",
            "Epoch 4/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0049\n",
            "Epoch 5/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0050\n",
            "Epoch 6/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0043\n",
            "Epoch 7/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0037\n",
            "Epoch 8/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0044\n",
            "Epoch 9/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0042\n",
            "Epoch 10/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0035\n",
            "Epoch 11/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0034\n",
            "Epoch 12/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0030\n",
            "Epoch 13/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0030\n",
            "Epoch 14/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0032\n",
            "Epoch 15/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0031\n",
            "Epoch 16/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0028\n",
            "Epoch 17/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0027\n",
            "Epoch 18/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0026\n",
            "Epoch 19/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0024\n",
            "Epoch 20/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0027\n",
            "Epoch 21/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0026\n",
            "Epoch 22/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0026\n",
            "Epoch 23/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0026\n",
            "Epoch 24/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0027\n",
            "Epoch 25/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0026\n",
            "Epoch 26/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0030\n",
            "Epoch 27/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0028\n",
            "Epoch 28/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0024\n",
            "Epoch 29/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0025\n",
            "Epoch 30/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0026\n",
            "Epoch 31/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0025\n",
            "Epoch 32/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0025\n",
            "Epoch 33/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0026\n",
            "Epoch 34/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0025\n",
            "Epoch 35/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0026\n",
            "Epoch 36/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0029\n",
            "Epoch 37/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0026\n",
            "Epoch 38/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0028\n",
            "Epoch 39/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0023\n",
            "Epoch 40/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0023\n",
            "Epoch 41/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0024\n",
            "Epoch 42/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0026\n",
            "Epoch 43/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0025\n",
            "Epoch 44/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0024\n",
            "Epoch 45/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0025\n",
            "Epoch 46/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0022\n",
            "Epoch 47/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0023\n",
            "Epoch 48/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0022\n",
            "Epoch 49/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0026\n",
            "Epoch 50/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0023\n",
            "Epoch 51/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0023\n",
            "Epoch 52/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0023\n",
            "Epoch 53/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0026\n",
            "Epoch 54/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0027\n",
            "Epoch 55/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0022\n",
            "Epoch 56/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0026\n",
            "Epoch 57/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0022\n",
            "Epoch 58/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0025\n",
            "Epoch 59/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0027\n",
            "Epoch 60/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0028\n",
            "Epoch 61/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0024\n",
            "Epoch 62/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0023\n",
            "Epoch 63/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0024\n",
            "Epoch 64/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0024\n",
            "Epoch 65/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0026\n",
            "Epoch 66/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0027\n",
            "Epoch 67/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0023\n",
            "Epoch 68/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0023\n",
            "Epoch 69/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0023\n",
            "Epoch 70/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0023\n",
            "Epoch 71/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0022\n",
            "Epoch 72/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0024\n",
            "Epoch 73/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0022\n",
            "Epoch 74/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0025\n",
            "Epoch 75/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0024\n",
            "Epoch 76/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0027\n",
            "Epoch 77/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0023\n",
            "Epoch 78/80\n",
            "94/94 [==============================] - 1s 11ms/step - loss: 0.0024\n",
            "Epoch 79/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0027\n",
            "Epoch 80/80\n",
            "94/94 [==============================] - 1s 12ms/step - loss: 0.0025\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_4 (LSTM)               (None, 7, 60)             14880     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 7, 60)             0         \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 120)               86880     \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 120)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 20)                2420      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 104,201\n",
            "Trainable params: 104,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 1s 771ms/step\n"
          ]
        }
      ],
      "source": [
        "# GET PREDICTIONS\n",
        "predictions = []\n",
        "\n",
        "for step in LOOKUP_STEPS:\n",
        "  df, last_sequence, x_train, y_train = PrepareData(step)\n",
        "  x_train = x_train[:, :, :len(['scaled_close'])].astype(np.float32)\n",
        "\n",
        "  model = GetTrainedModel(x_train, y_train)\n",
        "\n",
        "  last_sequence = last_sequence[-N_STEPS:]\n",
        "  last_sequence = np.expand_dims(last_sequence, axis=0)\n",
        "  prediction = model.predict(last_sequence)\n",
        "  predicted_price = scaler.inverse_transform(prediction)[0][0]\n",
        "\n",
        "  predictions.append(round(float(predicted_price), 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC065byaWJJS",
        "outputId": "d553aebf-157f-497d-e0f3-c070158cfd8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " prediction for upcoming 3 days (90.7$, 92.04$, 90.76$)\n"
          ]
        }
      ],
      "source": [
        "if bool(predictions) == True and len(predictions) > 0:\n",
        "  predictions_list = [str(d)+'$' for d in predictions]\n",
        "  predictions_str = ', '.join(predictions_list)\n",
        "  message = f' prediction for upcoming 3 days ({predictions_str})'\n",
        "  \n",
        "  print(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "D29dx0tOD6i4",
        "outputId": "e2e4c989-4d45-4a22-ca1f-0ba295b8c3ad"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2b00a174-5e38-496b-ad86-f57a9939f3e6\", \"goog_lstm.csv\", 97)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "today=start_date =datetime.datetime.today()\n",
        "day1=(today+datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "day2=(today+datetime.timedelta(days=2)).strftime('%Y-%m-%d')\n",
        "day3=(today+datetime.timedelta(days=3)).strftime('%Y-%m-%d')\n",
        "days=[day1,day2,day3]\n",
        "goog_lstm=pd.DataFrame()\n",
        "goog_lstm['future days']=days\n",
        "goog_lstm['price']=predictions\n",
        "goog_lstm['ticker']='GOOG'\n",
        "\n",
        "goog_lstm.to_csv('goog_lstm.csv') \n",
        "files.download('goog_lstm.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}